apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  timeout: 20m
  
  install:
    crds: Skip
    remediation:
      retries: 3
  
  upgrade:
    crds: Skip
    remediation:
      retries: 3
  
  chart:
    spec:
      chart: kube-prometheus-stack
      version: '82.x'
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      interval: 12h
  
  # Values defined inline instead of ConfigMap
  values:
    crds:
      enabled: false

    # Prometheus Operator configuration
    prometheusOperator:
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

    # Prometheus configuration
    prometheus:
      prometheusSpec:

        # Disable default PrometheusRules, ServiceMonitors and PodMonitors selection and pick up all
        ruleSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        ruleNamespaceSelector: {}
        podMonitorNamespaceSelector: {}
        serviceMonitorNamespaceSelector: {}

        retention: 3d
        
        resources:
          requests:
            cpu: "500m"
            memory: "1.5Gi"
          limits:
            cpu: "2000m"
            memory: "2.5Gi"

        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: role/monitor
                      operator: In
                      values:
                        - "true"

        # Use Longhorn single-replica storage
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-slow-single-replica
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 20Gi
        
        # Auto-discover pods with prometheus.io annotations
        additionalScrapeConfigs:
        - job_name: 'kubernetes-pods'
          kubernetes_sd_configs:
          - role: pod
          relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
    
    # Grafana configuration
    grafana:
      # Use existing secret for admin credentials
      admin:
        existingSecret: grafana-admin-credentials
        userKey: admin-user
        passwordKey: admin-password
      
      # Plugins
      plugins:
        - grafana-piechart-panel
      
      resources:
        requests:
          cpu: "100m"
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: role/monitor
                    operator: In
                    values:
                      - "true"

      # Persistence
      persistence:
        enabled: true
        size: 5Gi
        storageClassName: longhorn-slow-single-replica
      
      # Service - ClusterIP for use with Traefik
      service:
        type: ClusterIP
      
      additionalDataSources:
      # Loki (logs)
      - name: Loki
        version: 1
        uid: loki
        type: loki
        access: proxy
        url: http://loki-gateway.monitoring.svc.cluster.local
        editable: false
        jsonData:
          maxLines: 1000
          derivedFields:
            - name: trace_id
              datasourceUid: tempo
              matcherType: "label"
              matcherRegex: "trace_id"
              url: "$${__value.raw}"
              urlDisplayLabel: "View Trace"

      # Tempo (traces)
      - name: Tempo
        version: 1
        uid: tempo
        type: tempo
        access: proxy
        url: http://tempo.monitoring.svc.cluster.local:3200
        editable: false
        jsonData:
          tracesToLogsV2:
            datasourceUid: loki
            spanStartTimeShift: "-5m"
            spanEndTimeShift: "5m"
            filterByTraceID: true
            tags:
              - key: namespace
                value: namespace
              - key: pod
                value: pod
              - key: container
                value: container

          tracesToMetrics:
            datasourceUid: Prometheus
            spanStartTimeShift: "-5m"
            spanEndTimeShift: "5m"
            tags:
              - key: service.name
                value: service
      
    # Node exporter configuration
    prometheus-node-exporter:
      extraArgs:
        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
        - --collector.hwmon
        - --collector.thermal_zone
      resources:
        requests:
          cpu: "25m"
          memory: "64Mi"
        limits:
          cpu: "100m"
          memory: "128Mi"
    
    # kube-state-metrics
    kube-state-metrics:
      enabled: true
      resources:
        requests:
          cpu: "100m"
          memory: "128Mi"
        limits:
          cpu: "300m"
          memory: "256Mi"
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: role/monitor
                    operator: In
                    values:
                      - "true"
    
    # Alertmanager
    alertmanager:
      enabled: true
      alertmanagerSpec:
        alertmanagerConfigNamespaceSelector: {}
        alertmanagerConfigSelector: {}
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn-slow-single-replica
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 5Gi
        resources:
          requests:
            cpu: "50m"
            memory: "128Mi"
          limits:
            cpu: "200m"
            memory: "256Mi"
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: role/monitor
                      operator: In
                      values:
                        - "true"